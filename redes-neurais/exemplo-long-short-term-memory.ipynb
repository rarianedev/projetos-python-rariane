{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5b50344-0d37-4581-be52-f0f3f9a28d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X: (23, 3, 26) Shape y: (23, 26)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">7,552</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">858</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m7,552\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m)                  │             \u001b[38;5;34m858\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,410</span> (32.85 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,410\u001b[0m (32.85 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,410</span> (32.85 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,410\u001b[0m (32.85 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000023246F305E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n",
      "A rede acha que depois de 'def' vem 'g'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Exemplo simples: prever a letra seguinte em 'abcd...' usando Keras.\n",
    "Precisamos do TensorFlow instalado: `pip install tensorflow` (versão 2.15+).\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# ----- Gera dados toy: sequências 'abc' -> 'd', 'bcd' -> 'e', etc. -----\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "seq_length = 3\n",
    "\n",
    "def make_dataset(alphabet, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(alphabet) - seq_length):\n",
    "        seq_in  = alphabet[i:i+seq_length]\n",
    "        seq_out = alphabet[i+seq_length]\n",
    "        # one‑hot para letras\n",
    "        X.append([ord(c) - ord('a') for c in seq_in])\n",
    "        y.append(ord(seq_out) - ord('a'))\n",
    "    X = to_categorical(X, num_classes=len(alphabet))  # shape: (samples, seq_len, 26)\n",
    "    y = to_categorical(y, num_classes=len(alphabet))  # shape: (samples, 26)\n",
    "    return X, y\n",
    "\n",
    "X, y = make_dataset(alphabet, seq_length)\n",
    "print(\"Shape X:\", X.shape, \"Shape y:\", y.shape)  # (23, 3, 26) e (23, 26)\n",
    "\n",
    "# ----------------------- Modelo LSTM simples ---------------------------\n",
    "model = Sequential([\n",
    "    LSTM(32, input_shape=(seq_length, len(alphabet))),\n",
    "    Dense(len(alphabet), activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# ---------------------- Treinamento relâmpago --------------------------\n",
    "model.fit(X, y, epochs=300, verbose=0)\n",
    "\n",
    "# ----------------------- Teste: prever 'def' -> ? ----------------------\n",
    "test_seq = \"def\"\n",
    "test_in  = to_categorical([[ord(c) - ord('a') for c in test_seq]], 26)\n",
    "pred_idx = model.predict(test_in)[0].argmax()\n",
    "print(f\"A rede acha que depois de '{test_seq}' vem '{chr(pred_idx + ord('a'))}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91117ad9-17ff-42f6-8cc2-7e4a80e90811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Começo da simulação ===\n",
      "\n",
      "Passo t=0\n",
      "x_t:\n",
      " [-1.32818605  0.19686124  0.73846658  0.17136828]\n",
      "f_t (esquece): [0.70830745 0.56245234]\n",
      "i_t (entrada): [0.17153575 0.5884803 ]\n",
      "ĉ_t (candidato): [-0.20668741 -0.36109718]\n",
      "c_t (memória): [-0.03545428 -0.21249857]\n",
      "o_t (saída gate): [0.6484427  0.61176091]\n",
      "h_t (saída bloco): [-0.02298044 -0.12807631]\n",
      "\n",
      "Passo t=1\n",
      "x_t:\n",
      " [-0.11564828 -0.3011037  -1.47852199 -0.71984421]\n",
      "f_t (esquece): [0.24205837 0.72028517]\n",
      "i_t (entrada): [0.83199007 0.64240125]\n",
      "ĉ_t (candidato): [0.52965735 0.27763894]\n",
      "c_t (memória): [0.43208765 0.02529603]\n",
      "o_t (saída gate): [0.44804946 0.70727141]\n",
      "h_t (saída bloco): [0.18238503 0.01788734]\n",
      "\n",
      "Passo t=2\n",
      "x_t:\n",
      " [-0.46063877  1.05712223  0.34361829 -1.76304016]\n",
      "f_t (esquece): [0.71795705 0.09985228]\n",
      "i_t (entrada): [0.50434914 0.69494626]\n",
      "ĉ_t (candidato): [0.2090436  0.39114117]\n",
      "c_t (memória): [0.41565134 0.27434796]\n",
      "o_t (saída gate): [0.39326225 0.18115969]\n",
      "h_t (saída bloco): [0.15465455 0.04849028]\n",
      "\n",
      " [[0.15465455]\n",
      " [0.04849028]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --------------------- Hipóteses do \"mini‑problema\" --------------------\n",
    "\n",
    "#Os números são o “passo a passo” de um LSTM feito à mão: cada linha mostra como a\n",
    "#célula decide o que esquecer, o que entrar na memória e o que sai.\n",
    "\n",
    "np.random.seed(42)\n",
    "seq_len      = 3   # 3 passos de tempo (ex.: três palavras)\n",
    "input_dim    = 4   # tamanho do vetor que representa cada palavra\n",
    "hidden_dim   = 2   # quantos neurônios na célula LSTM\n",
    "\n",
    "# ------------- Pesos (aleatórios) para demonstrar o cálculo ------------\n",
    "def glorot(shape):\n",
    "    limit = np.sqrt(6 / sum(shape))\n",
    "    return np.random.uniform(-limit, limit, shape)\n",
    "\n",
    "W_f = glorot((hidden_dim, input_dim))   # pesos da porta de esquecer\n",
    "U_f = glorot((hidden_dim, hidden_dim))\n",
    "b_f = np.zeros((hidden_dim, 1))\n",
    "\n",
    "W_i = glorot((hidden_dim, input_dim))   # pesos da porta de entrada\n",
    "U_i = glorot((hidden_dim, hidden_dim))\n",
    "b_i = np.zeros((hidden_dim, 1))\n",
    "\n",
    "W_c = glorot((hidden_dim, input_dim))   # pesos do vetor candidato\n",
    "U_c = glorot((hidden_dim, hidden_dim))\n",
    "b_c = np.zeros((hidden_dim, 1))\n",
    "\n",
    "W_o = glorot((hidden_dim, input_dim))   # pesos da porta de saída\n",
    "U_o = glorot((hidden_dim, hidden_dim))\n",
    "b_o = np.zeros((hidden_dim, 1))\n",
    "\n",
    "# ----------------------- Ativação e utilitários ------------------------\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "# --------------------------- Dados de entrada --------------------------\n",
    "X = np.random.randn(seq_len, input_dim, 1)  # (t, features, batch=1)\n",
    "\n",
    "# -------------------- Vetores de estado inicial ------------------------\n",
    "h_prev = np.zeros((hidden_dim, 1))  # saída anterior\n",
    "c_prev = np.zeros((hidden_dim, 1))  # memória anterior\n",
    "\n",
    "print(\"=== Começo da simulação ===\")\n",
    "for t in range(seq_len):\n",
    "    x_t = X[t]\n",
    "\n",
    "    # 1️⃣ Porta de Esquecer (forget gate)\n",
    "    f_t = sigmoid(W_f @ x_t + U_f @ h_prev + b_f)\n",
    "\n",
    "    # 2️⃣ Porta de Entrada (input gate)\n",
    "    i_t = sigmoid(W_i @ x_t + U_i @ h_prev + b_i)\n",
    "\n",
    "    # 3️⃣ Vetor Candidato (valores novos que *podem* entrar na memória)\n",
    "    ĉ_t = np.tanh(W_c @ x_t + U_c @ h_prev + b_c)\n",
    "\n",
    "    # 4️⃣ Atualização da memória\n",
    "    c_t = f_t * c_prev + i_t * ĉ_t\n",
    "\n",
    "    # 5️⃣ Porta de Saída\n",
    "    o_t = sigmoid(W_o @ x_t + U_o @ h_prev + b_o)\n",
    "\n",
    "    # 6️⃣ Saída final do bloco LSTM\n",
    "    h_t = o_t * np.tanh(c_t)\n",
    "\n",
    "    # ---------- Impressões pedagógicas ----------\n",
    "    print(f\"\\nPasso t={t}\")\n",
    "    print(\"x_t:\\n\", x_t.ravel())\n",
    "    print(\"f_t (esquece):\", f_t.ravel())\n",
    "    print(\"i_t (entrada):\", i_t.ravel())\n",
    "    print(\"ĉ_t (candidato):\", ĉ_t.ravel())\n",
    "    print(\"c_t (memória):\", c_t.ravel())\n",
    "    print(\"o_t (saída gate):\", o_t.ravel())\n",
    "    print(\"h_t (saída bloco):\", h_t.ravel())\n",
    "\n",
    "    # 7️⃣ Propaga para o próximo tempo\n",
    "    h_prev, c_prev = h_t, c_t\n",
    "\n",
    "print(\"\\n\", h_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
